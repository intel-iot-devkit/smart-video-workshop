{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a356b470",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial demonstrates how to convert a Squeezenet ONNX* model to Intermediate Representation (IR) format. \n",
    "The Model Optimizer helps to convert models from multiple different supported frameworks to IR, which is used with the Inference Engine. If a model is not one of the pre-converted models from [Open Model Zoo](https://github.com/opencv/open_model_zoo), it is a required step before inferencing with Inference Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87aade",
   "metadata": {},
   "source": [
    "ONNX* allows AI developers easily transfer models between different frameworks. Today, PyTorch* , Caffe2* , Apache MXNet* , Microsoft Cognitive Toolkit* and other tools are having ONNX* support. Refer the supported public ONNX topologies [ONNX topologies](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html#supported_public_onnx_topologies).\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter* Notebook customizations and all the required libraries already installed.</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ecd9f",
   "metadata": {},
   "source": [
    "# Download the model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) utility  to download some common inference models from the [Open Model Zoo](https://github.com/opencv/open_model_zoo). We will use the Model Downloader tool in next tutorial and in this tutorial, we will download a public model from internet. Similary you can also use your own model.\n",
    "\n",
    "Following cell will downlaod Squeezenet public model and extract in squeezenet folder in your working directory. The required model file is **squeezenet/model.onnx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO- https://s3.amazonaws.com/download.onnx/models/opset_8/squeezenet.tar.gz | tar xvz squeezenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a036c1",
   "metadata": {},
   "source": [
    "# Convert the ONNX model to IR format:\n",
    "\n",
    "If you have already installed Intel® Distribution of OpenVINO™ toolkit, the Model Optimizer will abe available in \\<INSTALL_DIR\\>/deployment_tools/model_optimizer directory. To convert an ONNX model you can use following command.\n",
    "\n",
    "python3 mo.py --input_model \\<input_model_path\\>.onnx --data_type \\<model precision\\> --model_name \\<model name\\>\n",
    "\n",
    "Here,\n",
    "- **--input_model:** Path for the input model\n",
    "- **--data_type:** Data type for all intermediate tensors and weights\n",
    "- **--model_name:** Name of the IR Model\n",
    "\n",
    "There are no ONNX* specific parameters, so only framework-agnostic parameters are available to convert your model. Following cell will convert the model to IR format with FP32 precision. It might take few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py --input_model squeezenet/model.onnx --data_type FP32 --model_name squeezenet_fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b746c78",
   "metadata": {},
   "source": [
    "Let's check the size of the FP32 precision squeezent IR model by running following cell. It will be around 43MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58beb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh squeezenet_fp32.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c8bb9",
   "metadata": {},
   "source": [
    "Following cell will convert the model to IR format with FP16 precision. It might take few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py --input_model squeezenet/model.onnx --data_type FP16 --model_name squeezenet_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ca6c0",
   "metadata": {},
   "source": [
    "Let's check the size of the FP16 precision squeezent IR model by running following cell. It will be around 2.5MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh squeezenet_fp16.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30720c",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully completed this excersie and created an optimized your model. These IR files will serve as an input to the Inference Engine in your AI application. In this excersise you have downloaded a  model from internet, as a next step try using another ONNX* model and optimze and convert it using the Model Optimizer. In the next tutorial we will learn to download a model from Open Model Zoo and optimize it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.4 LTS)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
