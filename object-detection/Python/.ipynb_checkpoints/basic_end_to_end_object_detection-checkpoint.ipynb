{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "\n",
    "# Object detection with Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "This tutorial uses a Single Shot MultiBox Detector (SSD) on a trained mobilenet-ssd* model to walk you through the basic steps of using two key components of the Intel® Distribution of OpenVINO™ toolkit: Model Optimizer and Inference Engine.\n",
    "\n",
    "Model Optimizer is a cross-platform command-line tool that takes pre-trained deep learning models and optimizes them for performance/space using conservative topology transformations. It performs static model analysis and adjusts deep learning models for optimal execution on end-point target devices.\n",
    "\n",
    "Inference is the process of using a trained neural network to interpret data, such as images. This lab feeds a short video of cars, frame-by-frame, to the Inference Engine which subsequently utilizes an optimized trained neural network to detect cars.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "### Part 1: Optimize a deep-learning model using the Model Optimizer (MO)\n",
    "\n",
    "In this section, you will use the Model Optimizer to convert a trained model to two Intermediate Representation (IR) files (one .bin and one .xml). The Inference Engine requires this model conversion so that it can use the IR as input and achieve optimum performance on Intel hardware.\n",
    "<br>\n",
    "\n",
    "#### 1. Create a directory to store IR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### 2. Run the Model Optimizer on the pretrained Caffe* model. This step generates one .xml file and one .bin file and place both files in the tutorial samples directory (located here: /object-detection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel\n",
      "\t- Path for generated IR: \t/opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32\n",
      "\t- IR output name: \tmobilenet-ssd\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \t[127,127,127]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t256.0\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
      "[ SUCCESS ] BIN file: /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
      "[ SUCCESS ] Total execution time: 4.94 seconds. \n"
     ]
    }
   ],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model /opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel -o /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32 --scale 256 --mean_values [127,127,127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "\n",
    "   Note: Although this tutorial uses Single Shot MultiBox Detector (SSD) on a trained mobilenet-ssd* model, the Inference Engine is compatible with other neural network architectures, such as AlexNet*, GoogleNet*, MxNet* etc.\n",
    "\n",
    "\n",
    "The Model Optimizer converts a pretrained Caffe* model to make it compatible with the Intel Inference Engine and optimizes it for Intel® architecture. These are the files you would include with your C++ application to apply inference to visual data.\n",
    "\n",
    "   Note: if you continue to train or make changes to the Caffe* model, you would then need to re-run the Model Optimizer on the updated model.\n",
    "\n",
    "#### 3. Navigate to the tutorial sample model directory and verify creation of the optimized model files (the IR files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet-ssd.bin  mobilenet-ssd.mapping  mobilenet-ssd.xml\r\n"
     ]
    }
   ],
   "source": [
    "! cd /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32 && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "You should see the following two files listed in this directory: mobilenet-ssd.xml and mobilenet-ssd.bin\n",
    "<br>\n",
    "\n",
    "### Part 2: Use the mobilenet-ssd* model and Inference Engine in an object detection application\n",
    "<br>\n",
    "\n",
    "#### 1. Open the sample app (main.cpp) in the editor of your choice to view the lines that call the Inference Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gedit /opt/intel/workshop/smart-video-workshop/object-detection/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "-   Line 72 — Plugin initialization for specified device and load extensions library if specified\n",
    "-   Line 77 — initializes the network object\n",
    "-   Line 98 — Loading IR to the plugin...\n",
    "-   Line 102 — Read and pre-process input image\n",
    "-   Line 177 — allocate output blobs\n",
    "\n",
    "\n",
    "#### 2. Close the source file\n",
    "\n",
    "#### 3. Source your environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\r\n"
     ]
    }
   ],
   "source": [
    "! /opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### 4. Download the test video file to the object-detection folder.\n",
    "\n",
    "Download the input video file in the following link\n",
    "[https://pixabay.com/en/videos/download/video-1900_source.mp4?attachment](https://pixabay.com/en/videos/download/video-1900_source.mp4?attachment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "Cars - 1900.mp4 file will get downloaded. <br>\n",
    "\n",
    "Put that file in the /opt/intel/workshop/smart-video-workshop/object-detection folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/intel/Downloads/Cars - 1900.mp4': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! mv ~/Downloads/Cars\\ -\\ 1900.mp4 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### 5. Run the sample application to use the Inference Engine on the test video\n",
    "\n",
    "The below command runs the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "\n",
    "   Note: If you get an error related to \"undefined reference to 'google::FlagRegisterer...\", try uninstalling libgflags-dev: sudo apt-get remove libgflags-dev\n",
    "\n",
    "#### 6. Display output\n",
    "<br>\n",
    "For simplicity of the code and in order to put more focus on the performance number, video rendering with rectangle boxes for detected objects has been separated from main.cpp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !python3 ROIviewer.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -l /opt/intel/workshop/smart-video-workshop/object-detection/pascal_voc_classes.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "You should see a video play with cars running on the highway and red bounding boxes around them.\n",
    "\n",
    "Here are the parameters used in the above command to run the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 18px\">\n",
    "    \n",
    "### Part 3: Run the example on different hardware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### 1. CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so  -d CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "You will see the total time it took to run the inference.\n",
    "\n",
    "#### 2. GPU\n",
    "\n",
    "Since you installed the OpenCL™ drivers to use the GPU, you can run the inference on GPU and compare the difference.\n",
    "\n",
    "Set target hardware as GPU with -d GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -d GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "The total time between CPU and GPU will vary depending on your system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
