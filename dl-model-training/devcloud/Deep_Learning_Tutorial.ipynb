{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Deep Learning Tutorial\n",
    "\n",
    "\n",
    "## MNIST Database - Handwritten digits (0-9)'\n",
    "\n",
    "\n",
    "On this tutorial we will use Python* to implement one Convolutional Neural Network - a simplified version of LeNet - that will recognized Handwritten digits. A project like this one, using the MNIST dataset is considered as the \"Hello World\" of Machine Learning.\n",
    "\n",
    "We will use Keras*, TensorFlow* and the MNIST database.\n",
    "\n",
    "According to the description on their website, \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\"*\n",
    "\n",
    "We will use TensorFlow as the backend for Keras. TensorFlow is an open source software library for high performance numerical computation.\n",
    "\n",
    "The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. MNIST database is also available as a Keras dataset, with 60k 28x28 images of the 10 digits along with a test set of 10k images, so it is very easy to import and use it on our code.\n",
    "\n",
    "One good visual and interactive reference on what we are developing can be found here. The basic difference between our code and this interactive sample is the number and size of convolutional and fully-connected layers (LeNet uses two of each, we will use a single one, to reduce training time). We also adjusted the layers size to balance between accuracy and training time. We are achieving 98,54% of accuracy with less than 2 minutes training time on an Intel® Core™ processor.\n",
    "\n",
    "This code can also be optimized by several ways to increase accuracy, and we would like to invite you to explore this later, changing the number of epochs, filters, fully-connected neurons and also including additional convolutional and fully connected layers. You can also use flattening, dropout and batch normalization layers. Other optimization techniques can also be applied, so feel free to use this tutorial code as a base to explore those optimization techniques.\n",
    "\n",
    "In a nutshell, the convolutional and pooling layers are responsible for extracting a set of features from the input images, and the fully-connected layers are responsible for classification.\n",
    "\n",
    "Convolutional layers applies a set of filters to the input image to extract important features from the image. The filters are small matrixes also called image kernels that can be repeatedly applied to the input image (\"sliding\" the filter on the image). You may already used those filters on traditional image processing applications such as GIMP (i.e. blurring, sharpening or embossing). This article gives a good overview on image kernels with some live experiments. Each filter will generate a new image that will be the input for the next layer, typically a pooling layer.\n",
    "\n",
    "Pooling layers reduces the spatial size of the image (downsampling), reducing the computation in the network and also controlling overfitting.\n",
    "\n",
    "Fully connected layers are traditional Neural Network layers.\n",
    "\n",
    "\n",
    "## Installing the Python* libraries and steps to be followed\n",
    "\n",
    "To run the exercise on devcloud, you need to do some preliminary steps to install the dependencies. Open a terminal and execute the following steps :-  \n",
    "\n",
    "- rm -rf ~/.local/\n",
    "\n",
    "- If you get permission denied error while executing the above step, change the permissions of the file and then delete it\n",
    "\n",
    "- If you get the message Device or Resource busy while executing  step a.), execute lsof +D /path which lists the open files       with their process id's..Then you can use kill -9 pid to kill that process.\n",
    "\n",
    "\n",
    " After executing the above steps, you will get Kernel error. To solve this restart the kernel from the main page.\n",
    "\n",
    " After executing pip3 install keras tensorflow as shown in the next steps , again restart the kernel to avoid import errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/1b/18/2e1ef121e5560ac24c7ac9e363aa5fa7006c40563c989e7211aba95b793a/Keras-2.3.0-py2.py3-none-any.whl\n",
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/7c/fb/7b2c5b3e85ad335b53ca67deb2ef4af574dc0a8759f43b7f45e15005e449/tensorflow-1.14.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting pyyaml (from keras)\n",
      "Collecting h5py (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/10/56/d5c53cd170529bb40cd7dd43e2b68944cb65a45f65ab4c78a68f4ac9e51e/h5py-2.10.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting six>=1.9.0 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting scipy>=0.14 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/7a/0e/3781e028d62a8422244582abd8f084e6314297026760587c85607f687bf3/scipy-1.3.1-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting numpy>=1.9.1 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/9b/21/2b18339d24a2f73dcefb2f10f48aff6182e16da83e3a612684443c6cfb29/numpy-1.17.2-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/9c/3e/05df91cc2d08eec88b1869962beb6b144755e7d4a7a6668e1b9f0f450037/protobuf-3.9.2-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "Collecting wheel>=0.26 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/42/69/4acc9bdf349db7258867b8af96112661fbce65034de1326d65ad14c96e50/grpcio-1.24.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.6.1->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/86/095d2f7829badc207c893dd4ac767e871f6cd547145df797ea26baea4e2e/setuptools-41.2.0-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Installing collected packages: pyyaml, numpy, six, h5py, keras-preprocessing, scipy, keras-applications, keras, setuptools, protobuf, astor, wrapt, wheel, termcolor, gast, grpcio, absl-py, tensorflow-estimator, google-pasta, werkzeug, markdown, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.8.0 astor-0.8.0 gast-0.3.2 google-pasta-0.1.7 grpcio-1.24.0 h5py-2.10.0 keras-2.3.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 numpy-1.17.2 protobuf-3.9.2 pyyaml-5.1.2 scipy-1.3.1 setuptools-41.2.0 six-1.12.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 werkzeug-0.16.0 wheel-0.33.6 wrapt-1.11.2\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install keras tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Run the tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u26212/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /home/u26212/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "2019-09-30 00:07:34.824930: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2019-09-30 00:07:34.852350: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000000000 Hz\n",
      "2019-09-30 00:07:34.852686: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a39550 executing computations on platform Host. Devices:\n",
      "2019-09-30 00:07:34.852724: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-09-30 00:07:34.899578: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "WARNING:tensorflow:From /home/u26212/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 18s 296us/step - loss: 0.2280 - accuracy: 0.9347 - val_loss: 0.0849 - val_accuracy: 0.9730\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 0.0692 - accuracy: 0.9803 - val_loss: 0.0561 - val_accuracy: 0.9816\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 19s 325us/step - loss: 0.0463 - accuracy: 0.9862 - val_loss: 0.0490 - val_accuracy: 0.9831\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 26s 426us/step - loss: 0.0339 - accuracy: 0.9897 - val_loss: 0.0443 - val_accuracy: 0.9841\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.0256 - accuracy: 0.9921 - val_loss: 0.0412 - val_accuracy: 0.9862\n",
      "10000/10000 [==============================] - 2s 187us/step\n",
      "Accuracy = 98.61999750137329%\n"
     ]
    }
   ],
   "source": [
    "! python3 Deep_Learning_Tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "## How the tutorial code works\n",
    "\n",
    "The complete code for this tutorial can be found here\n",
    "Importing the necessary objects from Keras*\n",
    "\n",
    "\n",
    "Sequential Network Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Core Layers:\n",
    "\n",
    "Dense: densely-connected NN layer, to be used as classification layer\n",
    "Flatten: layer to flatten the convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "\n",
    "## Convolutional Layers:\n",
    "\n",
    "Conv2D: 2D convolution Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "  \n",
    "## Pooling Layer:\n",
    "\n",
    "MaxPooling2D: Max pooling operation for spatial data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "### MNIST Dataset:\n",
    "\n",
    "Dataset of 60,000 28x28 handwritten images of the 10 digits, along with a test set of 10,000 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "    \n",
    "\n",
    "## Download and load the MNIST database\n",
    "\n",
    "This will load the MNIST Dataset on four different variables:\n",
    "\n",
    "    train_set: Dataset with the training data (60k elements)\n",
    "    train_classes: Dataset with the equivalent training classes (60k elements)\n",
    "    test_dataset: Dataset with test data (10k elements)\n",
    "    test_classes: Dataset with the equivalent test classes (10k elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, train_classes),(test_dataset, test_classes) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "- NOTE: only on the first run on your machine, this will download the MNIST Dataset.\n",
    "Adjust the datasets to TensorFlow*\n",
    "\n",
    "First step, we need to reduce the image channels, from 3 (color) to 1 (grayscale):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.reshape(train_dataset.shape[0], 28, 28, 1)\n",
    "test_dataset = test_dataset.reshape(test_dataset.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Second step, we will convert the data from int8 to float32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype('float32')\n",
    "test_dataset = test_dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Third step, we need to normalize the data to speed up processing time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset / 255\n",
    "test_dataset = test_dataset / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Forth step, convert the classes data from numerical to categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = utils.to_categorical(train_classes, 10)\n",
    "test_classes = utils.to_categorical(test_classes, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "Now the data is ready to be processed by the CNN.\n",
    "\n",
    "\n",
    "## Create our Convolutional Neural Network (CNN)\n",
    "\n",
    "It is very simple and easy to create Neural Networks with Keras. We basically create the network, add the necessary layers, compile and execute the training.\n",
    "\n",
    "First thing is to create a Sequential Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now add the input layer, a 2D convolutional layer with 32 filters, 3x3 filter kernel size, input shape of 28 x 28 x 1 (as we adjusted on the training dataset) and using Rectified Linear Unit (relu) as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/u26212/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "cnn.add(Conv2D(32, (3,3), input_shape = (28, 28, 1), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "NOTE: We need to inform the input_shape parameter only if the convolutional layer is the input layer (first CNN layer). If you add another layers later on, you don't need to use this parameter.\n",
    "\n",
    "We add one Pooling layer using the default 2x2 size. This means that this layer will reduce by half the input image in both spatial dimentions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPooling2D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "    \n",
    "\n",
    "At this point, a traditional LeNet network would add another two layers, one convolutional and one pooling, basically repeating the two lines of code we just created, (removing the input_shape from the first one). As explained before, to speed processing time and make it more easy to understand, we decided to use just the two layers we just created.\n",
    "<br>\n",
    "\n",
    "Now we need to convert the output of the polling layer from a matrix to a vector, to be used by the classification part of our neural network. We do that using on flattening layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "    \n",
    "\n",
    "Our data is now ready for the classification part of our neural network, that will be implemented using just two layers, one hidden layer and one output layer.\n",
    "\n",
    "\n",
    "The first classification layer will be a fully-connected layer with 128 neurons and using rectified linear unit as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We now add another fully-connected layer that will be our output layer. Please note that this layer has 10 neurons, because we have 10 classes on our dataset. The activation function user here is Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "    \n",
    "\n",
    "Before we train the model, we need to \"compile\" it to configure the learning process.\n",
    "\n",
    "We will compile the CNN using categorical crossentropy as the loss function, adam as the optimizer and using accuracy as the results evaluation metric that will be show on the end of each apoch and also on the end of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    " \n",
    " NOTE: Adam is a gradient descent optimization algorithm. A good introduction to Adam can be found here.\n",
    "\n",
    "Our CNN is now ready to be trained.\n",
    "\n",
    "\n",
    "## Training our CNN\n",
    "\n",
    "To train the CNN we call the Fit method. On this training we will define:\n",
    "\n",
    "- Training dataset and training classes: our training dataset and training classes adjusted on the beginning of this tutorial.\n",
    "- Batch size: number of samples to be used per each gradient update, in our case, 128 (default is 32).\n",
    "- epochs: number of epochs that will be used on the training, in our case, 5 (for time saving purposes).\n",
    "- validation_data: the dataset used to validate the training on the end of each epoch. Here is where we inform out test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 24s 397us/sample - loss: 0.2106 - acc: 0.9392 - val_loss: 0.0739 - val_acc: 0.9772\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 29s 478us/sample - loss: 0.0653 - acc: 0.9804 - val_loss: 0.0562 - val_acc: 0.9822\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 29s 480us/sample - loss: 0.0447 - acc: 0.9867 - val_loss: 0.0523 - val_acc: 0.9840\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 29s 477us/sample - loss: 0.0342 - acc: 0.9895 - val_loss: 0.0443 - val_acc: 0.9843\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 29s 477us/sample - loss: 0.0252 - acc: 0.9928 - val_loss: 0.0412 - val_acc: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ef846709e8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train_dataset, train_classes, batch_size = 128, epochs = 5, validation_data = (test_dataset, test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "It will take a few minutes to run, and it will inform you the progress on the console. Note that it will inform the evolution of the loss (loss:) and accuracy (acc:) during the execution of each epoch, and this data is computed using the training data, so it cannot be used to evaluate the improvement of the epoch on the overall accuracy.\n",
    "\n",
    "\n",
    "At the end of each epoch, Keras will use the test dataset we provided to evaluate the epoch results, and this data will be displayed as val_loss: and val_acc: and those are good parameters to follow on each epoch to see how the accuracy improves. In general, the more epochs you run, more accuracy you will have (and more time you will need to run the training), but increasing the number of epochs is just one drop on the ocean of possibilities we have to optimize our CNN.\n",
    "\n",
    "## Evaluating the training results\n",
    "\n",
    "The simplest way to evaluate the training results is to use the evaluate method. It will show the same data as we saw on val_loss and val_acc on the end of the last epoch, but now we can use this data. On our tutorial, we will just print it on the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.0412 - acc: 0.9867\n",
      "Accuracy = 98.66999983787537%\n"
     ]
    }
   ],
   "source": [
    "result = cnn.evaluate(test_dataset, test_classes)\n",
    "print ('Accuracy = ' + str(result[1] * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "To have detailed information about our network accuracy for each class, we can use one confusion matrix (a.k.a error matrix). Scikit-learn library can be used to do that and more information about it can be found here. We will not implement the confusion matrix on this tutorial, but there are several online samples on how to create a confusion matrix using Keras and Scikit-learn and also on how to interpret the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
